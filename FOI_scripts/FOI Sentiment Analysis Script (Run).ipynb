{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imported Libaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01minspect\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mIPython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdisplay\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m display \u001b[38;5;28;01mas\u001b[39;00m original_display\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      8\u001b[0m pd\u001b[38;5;241m.\u001b[39mset_option(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdisplay.max_columns\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m      9\u001b[0m pd\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mmode\u001b[38;5;241m.\u001b[39mchained_assignment \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\__init__.py:62\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m# let init-time option registration happen\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig_init\u001b[39;00m  \u001b[38;5;66;03m# pyright: ignore[reportUnusedImport] # noqa: F401\u001b[39;00m\n\u001b[1;32m---> 62\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;66;03m# dtype\u001b[39;00m\n\u001b[0;32m     64\u001b[0m     ArrowDtype,\n\u001b[0;32m     65\u001b[0m     Int8Dtype,\n\u001b[0;32m     66\u001b[0m     Int16Dtype,\n\u001b[0;32m     67\u001b[0m     Int32Dtype,\n\u001b[0;32m     68\u001b[0m     Int64Dtype,\n\u001b[0;32m     69\u001b[0m     UInt8Dtype,\n\u001b[0;32m     70\u001b[0m     UInt16Dtype,\n\u001b[0;32m     71\u001b[0m     UInt32Dtype,\n\u001b[0;32m     72\u001b[0m     UInt64Dtype,\n\u001b[0;32m     73\u001b[0m     Float32Dtype,\n\u001b[0;32m     74\u001b[0m     Float64Dtype,\n\u001b[0;32m     75\u001b[0m     CategoricalDtype,\n\u001b[0;32m     76\u001b[0m     PeriodDtype,\n\u001b[0;32m     77\u001b[0m     IntervalDtype,\n\u001b[0;32m     78\u001b[0m     DatetimeTZDtype,\n\u001b[0;32m     79\u001b[0m     StringDtype,\n\u001b[0;32m     80\u001b[0m     BooleanDtype,\n\u001b[0;32m     81\u001b[0m     \u001b[38;5;66;03m# missing\u001b[39;00m\n\u001b[0;32m     82\u001b[0m     NA,\n\u001b[0;32m     83\u001b[0m     isna,\n\u001b[0;32m     84\u001b[0m     isnull,\n\u001b[0;32m     85\u001b[0m     notna,\n\u001b[0;32m     86\u001b[0m     notnull,\n\u001b[0;32m     87\u001b[0m     \u001b[38;5;66;03m# indexes\u001b[39;00m\n\u001b[0;32m     88\u001b[0m     Index,\n\u001b[0;32m     89\u001b[0m     CategoricalIndex,\n\u001b[0;32m     90\u001b[0m     RangeIndex,\n\u001b[0;32m     91\u001b[0m     MultiIndex,\n\u001b[0;32m     92\u001b[0m     IntervalIndex,\n\u001b[0;32m     93\u001b[0m     TimedeltaIndex,\n\u001b[0;32m     94\u001b[0m     DatetimeIndex,\n\u001b[0;32m     95\u001b[0m     PeriodIndex,\n\u001b[0;32m     96\u001b[0m     IndexSlice,\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;66;03m# tseries\u001b[39;00m\n\u001b[0;32m     98\u001b[0m     NaT,\n\u001b[0;32m     99\u001b[0m     Period,\n\u001b[0;32m    100\u001b[0m     period_range,\n\u001b[0;32m    101\u001b[0m     Timedelta,\n\u001b[0;32m    102\u001b[0m     timedelta_range,\n\u001b[0;32m    103\u001b[0m     Timestamp,\n\u001b[0;32m    104\u001b[0m     date_range,\n\u001b[0;32m    105\u001b[0m     bdate_range,\n\u001b[0;32m    106\u001b[0m     Interval,\n\u001b[0;32m    107\u001b[0m     interval_range,\n\u001b[0;32m    108\u001b[0m     DateOffset,\n\u001b[0;32m    109\u001b[0m     \u001b[38;5;66;03m# conversion\u001b[39;00m\n\u001b[0;32m    110\u001b[0m     to_numeric,\n\u001b[0;32m    111\u001b[0m     to_datetime,\n\u001b[0;32m    112\u001b[0m     to_timedelta,\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;66;03m# misc\u001b[39;00m\n\u001b[0;32m    114\u001b[0m     Flags,\n\u001b[0;32m    115\u001b[0m     Grouper,\n\u001b[0;32m    116\u001b[0m     factorize,\n\u001b[0;32m    117\u001b[0m     unique,\n\u001b[0;32m    118\u001b[0m     value_counts,\n\u001b[0;32m    119\u001b[0m     NamedAgg,\n\u001b[0;32m    120\u001b[0m     array,\n\u001b[0;32m    121\u001b[0m     Categorical,\n\u001b[0;32m    122\u001b[0m     set_eng_float_format,\n\u001b[0;32m    123\u001b[0m     Series,\n\u001b[0;32m    124\u001b[0m     DataFrame,\n\u001b[0;32m    125\u001b[0m )\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparseDtype\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtseries\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m infer_freq\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\core\\api.py:28\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmissing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     17\u001b[0m     isna,\n\u001b[0;32m     18\u001b[0m     isnull,\n\u001b[0;32m     19\u001b[0m     notna,\n\u001b[0;32m     20\u001b[0m     notnull,\n\u001b[0;32m     21\u001b[0m )\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01malgorithms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     24\u001b[0m     factorize,\n\u001b[0;32m     25\u001b[0m     unique,\n\u001b[0;32m     26\u001b[0m     value_counts,\n\u001b[0;32m     27\u001b[0m )\n\u001b[1;32m---> 28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrays\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Categorical\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrays\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mboolean\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BooleanDtype\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrays\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfloating\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     31\u001b[0m     Float32Dtype,\n\u001b[0;32m     32\u001b[0m     Float64Dtype,\n\u001b[0;32m     33\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\core\\arrays\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrays\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ArrowExtensionArray\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrays\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      3\u001b[0m     ExtensionArray,\n\u001b[0;32m      4\u001b[0m     ExtensionOpsMixin,\n\u001b[0;32m      5\u001b[0m     ExtensionScalarOpsMixin,\n\u001b[0;32m      6\u001b[0m )\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrays\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mboolean\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BooleanArray\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\core\\arrays\\arrow\\__init__.py:5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrays\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maccessors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      2\u001b[0m     ListAccessor,\n\u001b[0;32m      3\u001b[0m     StructAccessor,\n\u001b[0;32m      4\u001b[0m )\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrays\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marray\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ArrowExtensionArray\n\u001b[0;32m      7\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArrowExtensionArray\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStructAccessor\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mListAccessor\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\core\\arrays\\arrow\\array.py:65\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrays\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     61\u001b[0m     ExtensionArray,\n\u001b[0;32m     62\u001b[0m     ExtensionArraySupportsAnyAll,\n\u001b[0;32m     63\u001b[0m )\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrays\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmasked\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseMaskedArray\n\u001b[1;32m---> 65\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrays\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstring_\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StringDtype\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mcom\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mindexers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     68\u001b[0m     check_array_indexer,\n\u001b[0;32m     69\u001b[0m     unpack_tuple_and_ellipses,\n\u001b[0;32m     70\u001b[0m     validate_indices,\n\u001b[0;32m     71\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\core\\arrays\\string_.py:44\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrays\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ExtensionArray\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrays\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfloating\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     41\u001b[0m     FloatingArray,\n\u001b[0;32m     42\u001b[0m     FloatingDtype,\n\u001b[0;32m     43\u001b[0m )\n\u001b[1;32m---> 44\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrays\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minteger\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     45\u001b[0m     IntegerArray,\n\u001b[0;32m     46\u001b[0m     IntegerDtype,\n\u001b[0;32m     47\u001b[0m )\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrays\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnumpy_\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NumpyExtensionArray\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconstruction\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m extract_array\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1360\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1331\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:935\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:995\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1091\u001b[0m, in \u001b[0;36mget_code\u001b[1;34m(self, fullname)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1190\u001b[0m, in \u001b[0;36mget_data\u001b[1;34m(self, path)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "from sqlalchemy import create_engine, inspect\n",
    "\n",
    "import inspect\n",
    "from IPython.display import display as original_display\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "# import geopandas as gpd\n",
    "# from geopy.geocoders import Nominatim\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "import re\n",
    "import string\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Script Specifc Imported Libaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import itertools\n",
    "\n",
    "import threading\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "from queue import Queue\n",
    "results_queue = Queue()\n",
    "\n",
    "\n",
    "\n",
    "import nltk\n",
    "nltk.data.path.append('C:/Users/jf79')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import spacy\n",
    "from spellchecker import SpellChecker\n",
    "# from wordsegment import load, segment\n",
    "from wordsegment import Segmenter\n",
    "from collections import Counter\n",
    "# from wordcloud import WordCloud\n",
    "\n",
    "# Initialize external tools\n",
    "spell = SpellChecker()\n",
    "custom_segmenter = Segmenter()\n",
    "custom_segmenter.load()  # Load data for wordsegment\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "vocabulary = [\n",
    "    'FOI','vapes','vape','riley'\n",
    "]\n",
    "custom_stopwords = {\n",
    "    'freedom','information','act','following','report',\n",
    "    'please','dear','yes','request','provide',\n",
    "    'hammersmith','fulham','like','email','requesting',\n",
    "    'see','attach','foi','attached','service','would','many',\n",
    "    'faithfully','number','council','authority','local','year',\n",
    "    'borough','name','could','data','within','question','including',\n",
    "    'copy','also','made','regarding','since','relating','requested',\n",
    "    'thank','confirm','sent','make','date','much','kind','regard',\n",
    "    'good','afternoon','evening','sir','madam','hello','per','correspondence',\n",
    "    'address','total','contact','look','forward','hearing','way','day','apologise',\n",
    "    'responding','grateful','necessary','period','list','time','related','detail',\n",
    "    'use','end','last'\n",
    "}\n",
    "month_stopwords = {\n",
    "    'january','february','march','april',\n",
    "    'may','june','july','august','september',\n",
    "    'october','november','december','jan','feb',\n",
    "    'mar','apr','may','jun','jul','aug','sep',\n",
    "    'oct','nov','dec','sept'\n",
    "}\n",
    "\n",
    "spell.word_frequency.load_words(vocabulary)\n",
    "for word in vocabulary:\n",
    "    custom_segmenter.unigrams[word] = 1e9\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.update(custom_stopwords)\n",
    "stop_words.update(month_stopwords)\n",
    "\n",
    "import gensim\n",
    "from gensim.models import CoherenceModel, LdaModel\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "import pickle\n",
    "import pyLDAvis\n",
    "\n",
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "# from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standard Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Functions #\n",
    "\n",
    "# Function to clean labels in any plot functions\n",
    "def clean_label(label):\n",
    "    return label.replace('_', ' ').title()\n",
    " \n",
    "# Function for getting the name of a Dataframe\n",
    "def get_var_name(var):\n",
    "    for name, value in globals().items():\n",
    "        if value is var:\n",
    "            return name\n",
    " \n",
    "# Function to validate the data in a Dataframe\n",
    "def validate_data(df, show_counts=True):\n",
    "    df_name = get_var_name(df)\n",
    "    print(f'#########################################################################################################################################################################################\\nDataFrame: {df_name}')\n",
    "    #snapshot the dataset\n",
    "    display(df)\n",
    "    #check for unique values\n",
    "    unique_counts = pd.DataFrame(df.nunique())\n",
    "    unique_counts = unique_counts.reset_index().rename(columns={0:'No. of Unique Values', 'index':'Field Name'})\n",
    "    print(\"Unique values per field:\")\n",
    "    pd.set_option('display.max_rows', None)\n",
    "    display(unique_counts)\n",
    "    pd.reset_option('display.max_rows')\n",
    "    #checking for duplicates\n",
    "    duplicate_count = df.duplicated().sum()\n",
    "    print(\"\\nNumber of duplicate rows:\")\n",
    "    print(duplicate_count,'\\n')\n",
    "    info = df.info(show_counts=show_counts)\n",
    "    display(info)\n",
    "    #summary stats\n",
    "    print(\"\\nSummary statistics:\")\n",
    "    display(df.describe())\n",
    "    print('End of data validation\\n#########################################################################################################################################################################################\\n')\n",
    " \n",
    "# Function to provide list for data sources as a dataframe when conducting analysis\n",
    "def header_list(df):\n",
    "    df_list_ = df.copy()\n",
    "    df_list = df_list_.columns.tolist()\n",
    "    df_list = pd.DataFrame(df_list)\n",
    "    new_header = df_list.iloc[0]  # Get the first row for the header\n",
    "    df_list = df_list[1:]  # Take the data less the header row\n",
    "    df_list.columns = new_header  # Set the header row as the df header\n",
    "    df_list.reset_index(drop=True, inplace=True)  # Reset index\n",
    "   \n",
    "    return df_list\n",
    " \n",
    "def query_data(schema, data):\n",
    "    # Define the SQL query\n",
    "    query = f'SELECT * FROM [{schema}].[{data}]'\n",
    " \n",
    "    # Load data into DataFrame\n",
    "    df = pd.read_sql(query, engine)\n",
    " \n",
    "    print(f'Successfully imported {data}')\n",
    "    # Display the DataFrame\n",
    "    return df\n",
    "\n",
    "def display(df):\n",
    "    # Attempt to get the name of the DataFrame from the caller's local variables\n",
    "    frame = inspect.currentframe().f_back\n",
    "    # Attempt to find the variable name corresponding to the DataFrame\n",
    "    name = \"Unnamed DataFrame\"\n",
    "    for var_name, var_value in frame.f_locals.items():\n",
    "        if var_value is df:\n",
    "            name = var_name\n",
    "            break\n",
    " \n",
    "    # If the name is not in the list to be excluded, print it\n",
    "    if name not in {'df', 'Unnamed DataFrame', 'unique_counts'}:\n",
    "        print(f\"DataFrame: {name}\")\n",
    "    # Always display the DataFrame regardless of the name\n",
    "    original_display(df)\n",
    "\n",
    "def unique_values(df, display_df=True):\n",
    "    # Extract unique values for each field and store them in a dictionary\n",
    "    unique_values = {col: df[col].unique() for col in df.columns}\n",
    "    # Find the maximum number of unique values\n",
    "    max_length = max(len(values) for values in unique_values.values())\n",
    "    # Create a dictionary for the new DataFrame with padded None values\n",
    "    unique_df_data = {}\n",
    "    for col, values in unique_values.items():\n",
    "        unique_df_data[col] = list(values) + [None] * (max_length - len(values))\n",
    "    # Create the new DataFrame\n",
    "    unique_df = pd.DataFrame(unique_df_data)\n",
    "    if display_df == True:\n",
    "        # Set display options to show all rows and display the DataFrame\n",
    "        pd.set_option('display.max_rows', None)\n",
    "        display(unique_df.head(100))\n",
    "        # Reset display options back to default\n",
    "        pd.reset_option('display.max_rows')\n",
    "\n",
    "def read_directory():\n",
    "    directory = os.getcwd()\n",
    "    files = os.listdir(os.getcwd())\n",
    "    print(f\"Your Current Directory is: {directory}\")\n",
    "    print(\"Files in: %s\" % (files))\n",
    "\n",
    "def export_to_csv(df):\n",
    "    df_name = get_var_name(df)\n",
    "    # Specify the directory and filename\n",
    "    directory = r\"C:\\Users\\jf79\\OneDrive - Office Shared Service\\Documents\\H&F Analysis\\Python CSV Repositry\"\n",
    "    file_path = f'{directory}\\\\{df_name}.csv'\n",
    "    # Export the DataFrame to the specified directory\n",
    "    df.to_csv(file_path, index=False)\n",
    "    print(f'Successfully exported {df_name} to CSV')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Script Specific Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Script Specific Functions #\n",
    "\n",
    "def get_pos_tags(text):\n",
    "    doc = nlp(text)\n",
    "    return {token.text: token.tag_ for token in doc}\n",
    "\n",
    "def is_valid_correction(original, pos_tags):\n",
    "    # Skip correction if the original word matches these patterns\n",
    "    if re.match(r\"\\d+(st|nd|rd|th)\", original):  # Ordinal numbers\n",
    "        return False\n",
    "    if pos_tags and pos_tags.get(original) == 'NNP' or pos_tags.get(original) == 'PROPN':\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def is_valid_token(word):\n",
    "    if len(word) > 2:\n",
    "        return True\n",
    "    if any(char.isdigit() for char in word):\n",
    "        return False\n",
    "    return False\n",
    "\n",
    "# Tokenization function\n",
    "def safe_tokenize(text):\n",
    "    if isinstance(text, str):\n",
    "        return word_tokenize(text)\n",
    "    return []\n",
    "\n",
    "def segementing_and_spelling(tokens):\n",
    "    # Step 3: Spelling correction and word segmentation\n",
    "    corrected_tokens = []\n",
    "    for word in tokens:\n",
    "        split_words = custom_segmenter.segment(word)  # Split merged words\n",
    "        for subword in split_words:\n",
    "            corrected_word = spell.correction(subword) or word # Correct spelling\n",
    "            pos_tags = get_pos_tags(subword)\n",
    "            if not is_valid_correction(subword, pos_tags):  # Skip invalid corrections\n",
    "                corrected_word = subword\n",
    "            corrected_tokens.append(corrected_word)\n",
    "        \n",
    "    filtered_tokens = [word for word in corrected_tokens if is_valid_token(word)]\n",
    "\n",
    "    \n",
    "    return filtered_tokens\n",
    "\n",
    "def process_text(row,details):\n",
    "    text = row[f'{details}']\n",
    "    if not isinstance(text, str):  # Convert non-string types to string\n",
    "        text = str(text)\n",
    "    if not text.strip():  # Skip empty or whitespace strings\n",
    "        return \"\"\n",
    " \n",
    "    try:\n",
    "        # Step 1: Text cleaning\n",
    "        text = text.lower()  # Lowercase\n",
    "        text = re.sub(r\"https?://\\S+|www\\.\\S+\", \"\", text)  # Remove URLs\n",
    "        text = re.sub(r\"\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\b\", \"\", text)  # Remove email addresses\n",
    "        text = re.sub(r\"\\d+\", \"\", text)  # Remove numbers\n",
    "        text = re.sub(r\"[^\\w\\s]\", \"\", text)  # Remove punctuation\n",
    "        text = re.sub(r\"\\s+\", \" \", text).strip()  # Normalize spaces\n",
    " \n",
    "        # Step 2: Tokenization\n",
    "        tokens = text.split()\n",
    "        \n",
    "        # Step 3 & 4: Word segmenting and spelling\n",
    "        corrected_tokens = segementing_and_spelling(tokens=tokens)\n",
    " \n",
    "        # Step 5: Lemmatization and stop word removal\n",
    "        lemmatized_tokens = [lemmatizer.lemmatize(word) for word in corrected_tokens if word not in stop_words]\n",
    "\n",
    "        lemmatized_tokens = [word for word in lemmatized_tokens if word not in stop_words]\n",
    "\n",
    "        # Step 6: Word segmenting and spelling (again)\n",
    "        final_tokens = segementing_and_spelling(tokens=lemmatized_tokens)\n",
    "\n",
    "        final_tokens = [word for word in final_tokens if word not in stop_words]\n",
    "\n",
    "        # Step 7: Join processed tokens into a string\n",
    "        return \" \".join(final_tokens)\n",
    " \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing text: {repr(text)}\")\n",
    "        raise e\n",
    " \n",
    " \n",
    "def process_text_with_progress(df,details):\n",
    "    # Initialize tqdm progress bar\n",
    "    pbar = tqdm.tqdm(total=len(df), desc=\"Processing rows\", unit=\"row\", dynamic_ncols=True)\n",
    "    def process_row(row):\n",
    "        try:\n",
    "            return process_text(row,details=details)\n",
    "        finally:\n",
    "            # Update the progress bar for every row processed\n",
    "            pbar.update(1)\n",
    "    \n",
    "    df['processed_details'] = df.apply(process_row, axis=1)\n",
    "    df['processed_tokens'] = df['processed_details'].apply(safe_tokenize)\n",
    "\n",
    "    pbar.close()\n",
    "    \n",
    "    return df\n",
    "\n",
    "def prepare_corpus(tokens, threshold=100, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    # Generate bigrams and trigrams\n",
    "    bigram = gensim.models.Phrases(tokens, min_count=5, threshold=threshold)\n",
    "    trigram = gensim.models.Phrases(bigram[tokens], threshold=threshold)\n",
    "    bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "    trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "    bigram_texts = (bigram_mod[doc] for doc in tokens)\n",
    "    trigram_texts = (trigram_mod[bigram_mod[doc]] for doc in bigram_texts)\n",
    "\n",
    "    # Verify POS of each word\n",
    "    texts_out = []\n",
    "    for text in trigram_texts:\n",
    "        # Use SpaCy to process the text\n",
    "        doc = nlp(\" \".join(text))  # Join tokens into a string and process with SpaCy\n",
    "        filtered_tokens = [token.text for token in doc if token.pos_ in allowed_postags]\n",
    "        texts_out.append(filtered_tokens)\n",
    "    return texts_out\n",
    "\n",
    "def count_words(tokens):\n",
    "    # Flatten the tokenized details\n",
    "    all_tokens = [token for sublist in tokens for token in sublist]\n",
    "    # Count word frequencies\n",
    "    word_counts = Counter(all_tokens)\n",
    "    # Print the top 10 most common words\n",
    "    word_counts = pd.DataFrame([word_counts])\n",
    "    word_counts.reset_index(inplace=True)\n",
    "    word_counts = word_counts.melt(id_vars='index',var_name='Word',value_name='Count')\n",
    "    word_counts.sort_values(by='Count',ascending=False,inplace=True)\n",
    "    word_counts.reset_index(inplace=True,drop=['index'])\n",
    "    word_counts.drop(columns='index',inplace=True)\n",
    "    display(word_counts.head(25))\n",
    "    display(word_counts)\n",
    "\n",
    "# Model Evaluation Function\n",
    "def compute_coherence_values(corpus, dictionary, k, a, b):\n",
    "    lda_model = gensim.models.LdaMulticore(\n",
    "        corpus=corpus,\n",
    "        id2word=dictionary,\n",
    "        num_topics=k,\n",
    "        random_state=42,\n",
    "        chunksize=100,\n",
    "        passes=10,\n",
    "        alpha=a,\n",
    "        eta=b,\n",
    "        workers=4\n",
    "    )\n",
    "    coherence_model_lda = CoherenceModel(model=lda_model, texts=FOI_corpus, dictionary=id2word, coherence='c_v')\n",
    "\n",
    "    return coherence_model_lda.get_coherence()\n",
    "\n",
    "# Define the function for computing coherence for parameter sets\n",
    "def compute_for_params(corpus_set, k, a, b, corpus_title):\n",
    "    try:\n",
    "        # Thread-local coherence value computation\n",
    "        cv = compute_coherence_values(corpus=corpus_sets[corpus_set], dictionary=id2word, k=k, a=a, b=b)\n",
    "        # The result is created with thread-local variables\n",
    "        result = {\n",
    "            'Validation_Set': corpus_title,\n",
    "            'Topics': k,\n",
    "            'Alpha': a,\n",
    "            'Beta': b,\n",
    "            'Coherence': cv\n",
    "        }\n",
    "        results_queue.put(result)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing (k={k}, a={a}, b={b}, corpus_title={corpus_title}): {str(e)}\")\n",
    "\n",
    "def run_io_task_in_parallel(tasks):\n",
    "    with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "        futures = [executor.submit(task) for task in tasks]\n",
    "        for future in futures:\n",
    "            future.result()\n",
    "\n",
    "def find_element(corpus_title):\n",
    "    if corpus_title == '75% Corpus':\n",
    "        return 0\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Database and CWD setup and connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database credentials\n",
    "db_host = 'LBHHLWSQL0001.lbhf.gov.uk'\n",
    "db_port = '1433'\n",
    "db_name = 'IA_ODS'\n",
    " \n",
    "# Create the connection string for SQL Server using pyodbc with Windows Authentication\n",
    "connection_string = f'mssql+pyodbc://@{db_host}:{db_port}/{db_name}?driver=ODBC+Driver+17+for+SQL+Server&Trusted_Connection=yes'\n",
    "\n",
    "# Create the database engine\n",
    "engine = create_engine(connection_string)\n",
    "\n",
    "# Define the current working directory\n",
    "cwd = r'C:\\Users\\jf79\\OneDrive - Office Shared Service\\Documents\\H&F Analysis\\Fuel Poverty Analysis\\Fuel Poverty General\\Fuel Bible'\n",
    "os.chdir(cwd)\n",
    "files = os.listdir(os.getcwd())\n",
    "print(\"Files in %r: %s\" % (cwd, files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file into a DataFrame\n",
    "cwd = r'C:\\Users\\jf79\\OneDrive - Office Shared Service\\Documents\\H&F Analysis\\FOI Sentiment Analysis\\Data'\n",
    "os.chdir(cwd)\n",
    "read_directory()\n",
    "\n",
    "FOI_export = pd.read_csv('FOI_export.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "FOI_final = FOI_export.copy()\n",
    "# FOI_final = FOI_final[FOI_final['processed_details'] != '']\n",
    "FOI_final['processed_tokens'] = FOI_final['processed_details'].apply(safe_tokenize)\n",
    "FOI_final['new_tokens'] = FOI_final['processed_tokens'].apply(lambda x: [word for word in x if word not in stop_words])\n",
    "# count_words(FOI_final['new_tokens'])\n",
    "display()\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "FOI_corpus = prepare_corpus(FOI_final['new_tokens'],threshold=50)\n",
    "\n",
    "# Create Dictionary\n",
    "id2word = Dictionary(FOI_corpus)\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in FOI_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize variables\n",
    "results = pd.DataFrame(columns=['Validation_Set', 'Topics', 'Alpha', 'Beta', 'Coherence'])\n",
    "lock = threading.Lock()  # Thread-safe lock\n",
    "\n",
    "# Number of documents in the corpus\n",
    "num_of_docs = len(corpus)\n",
    "\n",
    "# Prepare corpus sets (75% and 100% corpus)\n",
    "corpus_sets = [\n",
    "    gensim.utils.ClippedCorpus(corpus, int(num_of_docs * 0.75)),\n",
    "    corpus\n",
    "]\n",
    "\n",
    "# Hyper-parameters\n",
    "min_topics = 2\n",
    "max_topics = 15\n",
    "step_size = 1\n",
    "topics_range = list(range(min_topics,max_topics+1,step_size))\n",
    "alphas = np.round(np.linspace(0.01, 0.91, 5), 2).tolist() + ['symmetric', 'asymmetric']\n",
    "betas = np.round(np.linspace(0.01, 0.91, 5), 2).tolist() + ['symmetric']\n",
    "corpus_titles = ['75% Corpus', '100% Corpus']\n",
    "\n",
    "# Define test combinations to run in parallel\n",
    "test_combinations = [\n",
    "    (find_element(corpus_title),topic,alpha,beta,corpus_title)\n",
    "    for topic,alpha,beta,corpus_title in itertools.product(topics_range,alphas,betas,corpus_titles)\n",
    "]\n",
    "\n",
    "tasks = [lambda combination=combination: compute_for_params(*combination) for combination in test_combinations]\n",
    "# Process tasks in parallel with a progress bar\n",
    "with tqdm(total=len(test_combinations), desc='Processing combinations') as pbar:\n",
    "    def task_with_progress(task):\n",
    "        task()\n",
    "        pbar.update(1)\n",
    "    \n",
    "    run_io_task_in_parallel([lambda task=task: task_with_progress(task) for task in tasks])\n",
    "\n",
    "results = pd.DataFrame(columns=['Validation_Set','Topics','Alpha','Beta','Coherence'])\n",
    "while not results_queue.empty():\n",
    "    result = results_queue.get()\n",
    "    results = pd.concat([results, pd.DataFrame([result])], ignore_index=True)\n",
    "\n",
    "\n",
    "display(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r\n",
    "num_topics = 6\n",
    "# Build LDA model\n",
    "lda_model = gensim.models.LdaMulticore(\n",
    "    corpus=corpus,\n",
    "    id2word=id2word,\n",
    "    num_topics=num_topics,\n",
    "    random_state=42,\n",
    "    chunksize=100,\n",
    "    passes=10,\n",
    "    per_word_topics=True\n",
    ")\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=FOI_corpus, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('Coherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the topics\n",
    "pyLDAvis.enable_notebook()\n",
    "path = 'C:/Users/jf79/OneDrive - Office Shared Service/Documents/H&F Analysis/FOI Sentiment Analysis/Data'\n",
    "LDAvis_data_filepath = os.path.join(path+str(num_topics))\n",
    "\n",
    "# # this is a bit time consuming - make the if statement True\n",
    "# # if you want to execute visualization prep yourself\n",
    "if 1 == 1:\n",
    "    LDAvis_prepared = gensimvis.prepare(lda_model, corpus, id2word)\n",
    "    with open(LDAvis_data_filepath, 'wb') as f:\n",
    "        pickle.dump(LDAvis_prepared, f)\n",
    "\n",
    "# load the pre-prepared pyLDAvis data from disk\n",
    "with open(LDAvis_data_filepath, 'rb') as f:\n",
    "    LDAvis_prepared = pickle.load(f)\n",
    "\n",
    "pyLDAvis.save_html(LDAvis_prepared,path+str(num_topics)+'.html')\n",
    "\n",
    "LDAvis_prepared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambd = 0.2  # A specific relevance metric value\n",
    " \n",
    "all_topics = {}\n",
    "all_topics_with_docs = {}\n",
    "num_topics = lda_model.num_topics\n",
    "num_terms = 30\n",
    "num_docs = 5  # Number of most related documents to retrieve for each topic\n",
    " \n",
    "topic_doc_matrix = lda_model.get_document_topics(corpus, minimum_probability=0)  # Get the topic-document distribution\n",
    "print\n",
    "for i in range(1, num_topics + 1):  # Indices are 1-based, not 0-based\n",
    "    # Extract the term information\n",
    "    topic = LDAvis_prepared.topic_info[LDAvis_prepared.topic_info.Category == 'Topic' + str(i)].copy()\n",
    "    topic['relevance'] = topic['loglift'] * (1 - lambd) + topic['logprob'] * lambd\n",
    "    all_topics['Topic ' + str(i)] = topic.sort_values(by='relevance', ascending=False).Term[:num_terms].values\n",
    " \n",
    "    # Find the most related documents for the topic\n",
    "    topic_index = i - 1  # Convert to 0-based index\n",
    "    doc_relevance = [doc[topic_index][1] for doc in topic_doc_matrix]  # Extract the relevance of documents for this topic\n",
    "    top_doc_indices = sorted(range(len(doc_relevance)), key=lambda x: doc_relevance[x], reverse=True)[:num_docs]\n",
    "\n",
    "    # Retrieve the actual document texts\n",
    "    top_docs_content = [FOI_final['details'][idx] for idx in top_doc_indices]  # Assuming `documents` contains the original texts\n",
    "    all_topics_with_docs['Topic ' + str(i)] = {\n",
    "        'terms': all_topics['Topic ' + str(i)],\n",
    "        'top_docs_indices': top_doc_indices,  # Indice of most relevant doc\n",
    "        'top_docs': top_docs_content  # Contents of the most relevant documents\n",
    "    }\n",
    " \n",
    "# Create a DataFrame to display terms and documents for each topic\n",
    "output_data = {\n",
    "    'Topic': [],\n",
    "    'Top Terms': [],\n",
    "    'Top Doc Indices': [],\n",
    "    'Top Documents': []\n",
    "}\n",
    " \n",
    "for topic, data in all_topics_with_docs.items():\n",
    "    output_data['Topic'].append(topic)\n",
    "    output_data['Top Terms'].append(\", \".join(data['terms']))\n",
    "    output_data['Top Doc Indices'].append(\", \".join(map(str,data['top_docs_indices'])))\n",
    "    output_data['Top Documents'].append(\" || \".join(data['top_docs']))  # Separate documents with '||'\n",
    "\n",
    "pd.DataFrame(output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_doc_matrix = lda_model.get_document_topics(corpus, minimum_probability=0)  # Get the topic-document distribution\n",
    "for i in range(1, num_topics + 1):  # Indices are 1-based, not 0-based\n",
    "    # Find the most related documents for the topic\n",
    "    topic_index = i - 1  # Convert to 0-based index\n",
    "    doc_relevance = [doc[topic_index][1] for doc in topic_doc_matrix]  # Extract the relevance of documents for this topic\n",
    "    top_doc_indices = sorted(range(len(doc_relevance)), key=lambda x: doc_relevance[x], reverse=True)[:num_docs]\n",
    "    print(doc_relevance[0])\n",
    "    print(f'{topic_index}: {topic_doc_matrix[0][topic_index][1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_relevance = [doc[topic_index][1] for doc in topic_doc_matrix]\n",
    "topic_doc_matrix[0][0][1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
