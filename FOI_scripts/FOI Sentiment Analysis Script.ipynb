{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imported Libaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sqlalchemy import create_engine, inspect\n",
    "\n",
    "import inspect\n",
    "from IPython.display import display as original_display\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import re\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\jf79\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\jf79\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\jf79\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.data.path.append('C:/Users/jf79')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import spacy\n",
    "from spellchecker import SpellChecker\n",
    "from wordsegment import load, segment\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Initialize external tools\n",
    "spell = SpellChecker()\n",
    "load()  # Load data for wordsegment\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "vocabulary = [\n",
    "    'FOI'\n",
    "]\n",
    "custom_stopwords = {\n",
    "    'freedom','information','act','following','report',\n",
    "    'please','dear','regard','yes','request','provide',\n",
    "    'hammersmith','fulham','like','email','requesting',\n",
    "    'see','attach'\n",
    "}\n",
    "for word in vocabulary:\n",
    "    # Add custom vocabulary to SpellChecker\n",
    "    spell.word_frequency.add(word)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.update(custom_stopwords)\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standard Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Functions #\n",
    "\n",
    "# Function to clean labels in any plot functions\n",
    "def clean_label(label):\n",
    "    return label.replace('_', ' ').title()\n",
    " \n",
    "# Function for getting the name of a Dataframe\n",
    "def get_var_name(var):\n",
    "    for name, value in globals().items():\n",
    "        if value is var:\n",
    "            return name\n",
    " \n",
    "# Function to validate the data in a Dataframe\n",
    "def validate_data(df, show_counts=True):\n",
    "    df_name = get_var_name(df)\n",
    "    print(f'#########################################################################################################################################################################################\\nDataFrame: {df_name}')\n",
    "    #snapshot the dataset\n",
    "    display(df)\n",
    "    #check for unique values\n",
    "    unique_counts = pd.DataFrame(df.nunique())\n",
    "    unique_counts = unique_counts.reset_index().rename(columns={0:'No. of Unique Values', 'index':'Field Name'})\n",
    "    print(\"Unique values per field:\")\n",
    "    pd.set_option('display.max_rows', None)\n",
    "    display(unique_counts)\n",
    "    pd.reset_option('display.max_rows')\n",
    "    #checking for duplicates\n",
    "    duplicate_count = df.duplicated().sum()\n",
    "    print(\"\\nNumber of duplicate rows:\")\n",
    "    print(duplicate_count,'\\n')\n",
    "    info = df.info(show_counts=show_counts)\n",
    "    display(info)\n",
    "    #summary stats\n",
    "    print(\"\\nSummary statistics:\")\n",
    "    display(df.describe())\n",
    "    print('End of data validation\\n#########################################################################################################################################################################################\\n')\n",
    " \n",
    "# Function to provide list for data sources as a dataframe when conducting analysis\n",
    "def header_list(df):\n",
    "    df_list_ = df.copy()\n",
    "    df_list = df_list_.columns.tolist()\n",
    "    df_list = pd.DataFrame(df_list)\n",
    "    new_header = df_list.iloc[0]  # Get the first row for the header\n",
    "    df_list = df_list[1:]  # Take the data less the header row\n",
    "    df_list.columns = new_header  # Set the header row as the df header\n",
    "    df_list.reset_index(drop=True, inplace=True)  # Reset index\n",
    "   \n",
    "    return df_list\n",
    " \n",
    "def query_data(schema, data):\n",
    "    # Define the SQL query\n",
    "    query = f'SELECT * FROM [{schema}].[{data}]'\n",
    " \n",
    "    # Load data into DataFrame\n",
    "    df = pd.read_sql(query, engine)\n",
    " \n",
    "    print(f'Successfully imported {data}')\n",
    "    # Display the DataFrame\n",
    "    return df\n",
    "\n",
    "def display(df):\n",
    "    # Attempt to get the name of the DataFrame from the caller's local variables\n",
    "    frame = inspect.currentframe().f_back\n",
    "    # Attempt to find the variable name corresponding to the DataFrame\n",
    "    name = \"Unnamed DataFrame\"\n",
    "    for var_name, var_value in frame.f_locals.items():\n",
    "        if var_value is df:\n",
    "            name = var_name\n",
    "            break\n",
    " \n",
    "    # If the name is not in the list to be excluded, print it\n",
    "    if name not in {'df', 'Unnamed DataFrame', 'unique_counts'}:\n",
    "        print(f\"DataFrame: {name}\")\n",
    "    # Always display the DataFrame regardless of the name\n",
    "    original_display(df)\n",
    "\n",
    "def unique_values(df, display_df=True):\n",
    "    # Extract unique values for each field and store them in a dictionary\n",
    "    unique_values = {col: df[col].unique() for col in df.columns}\n",
    "    # Find the maximum number of unique values\n",
    "    max_length = max(len(values) for values in unique_values.values())\n",
    "    # Create a dictionary for the new DataFrame with padded None values\n",
    "    unique_df_data = {}\n",
    "    for col, values in unique_values.items():\n",
    "        unique_df_data[col] = list(values) + [None] * (max_length - len(values))\n",
    "    # Create the new DataFrame\n",
    "    unique_df = pd.DataFrame(unique_df_data)\n",
    "    if display_df == True:\n",
    "        # Set display options to show all rows and display the DataFrame\n",
    "        pd.set_option('display.max_rows', None)\n",
    "        display(unique_df.head(100))\n",
    "        # Reset display options back to default\n",
    "        pd.reset_option('display.max_rows')\n",
    "\n",
    "def read_directory():\n",
    "    directory = os.getcwd()\n",
    "    files = os.listdir(os.getcwd())\n",
    "    print(f\"Your Current Directory is: {directory}\")\n",
    "    print(\"Files in: %s\" % (files))\n",
    "\n",
    "def export_to_csv(df):\n",
    "    df_name = get_var_name(df)\n",
    "    # Specify the directory and filename\n",
    "    directory = r\"C:\\Users\\jf79\\OneDrive - Office Shared Service\\Documents\\H&F Analysis\\Python CSV Repositry\"\n",
    "    file_path = f'{directory}\\\\{df_name}.csv'\n",
    "    # Export the DataFrame to the specified directory\n",
    "    df.to_csv(file_path, index=False)\n",
    "    print(f'Successfully exported {df_name} to CSV')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Script Specific Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Script Specific Functions #\n",
    "\n",
    "def get_pos_tags(text):\n",
    "    doc = nlp(text)\n",
    "    return {token.text: token.tag_ for token in doc}\n",
    "\n",
    "def is_valid_correction(original, pos_tags):\n",
    "    # Skip correction if the original word matches these patterns\n",
    "    if re.match(r\"\\d+(st|nd|rd|th)\", original):  # Ordinal numbers\n",
    "        return False\n",
    "    if pos_tags and pos_tags.get(original) == 'NNP':\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def is_valid_token(word):\n",
    "    if len(word) > 2:\n",
    "        return True\n",
    "    if any(char.isdigit() for char in word):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def process_text(text):\n",
    "    if not isinstance(text, str):  # Convert non-string types to string\n",
    "        text = str(text)\n",
    "    if not text.strip():  # Skip empty or whitespace strings\n",
    "        return \"\"\n",
    "    \n",
    "    try:\n",
    "        # Lowercase and clean text\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)  # Remove URLs\n",
    "        text = re.sub(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\b', '', text)  # Remove email addresses\n",
    "        text = re.sub(r'\\s+', ' ', text)  # Normalize spaces after removal\n",
    "        text = re.sub(r'[^\\w\\s]', '', text) # Remove puntuctation\n",
    "        tokens = text.split()\n",
    "\n",
    "        # Correct spelling and segment merged words\n",
    "        corrected_tokens = []\n",
    "        for word in tokens:\n",
    "            split_words = segment(word)  # Split merged words\n",
    "            for subword in split_words:\n",
    "                corrected_word = spell.correction(subword) or word  # Correct spelling\n",
    "                pos_tags = get_pos_tags(subword)\n",
    "                if not is_valid_correction(subword, pos_tags):  # Skip invalid corrections\n",
    "                    corrected_word = subword\n",
    "                subword = corrected_word\n",
    "            corrected_tokens.extend(split_words)\n",
    "        \n",
    "        # Remove single letter words\n",
    "        corrected_tokens = [word for word in corrected_tokens if is_valid_token(word)]\n",
    "\n",
    "        # Lemmatize and remove stop words\n",
    "        tokens = [\n",
    "            lemmatizer.lemmatize(word)\n",
    "            for word in corrected_tokens\n",
    "            if word not in stop_words\n",
    "        ]\n",
    "        return ' '.join(tokens)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing text: {repr(text)}\")\n",
    "        raise e\n",
    "\n",
    "# Tokenization function\n",
    "def safe_tokenize(text):\n",
    "    if isinstance(text, str):\n",
    "        return word_tokenize(text)\n",
    "    return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Database and CWD setup and connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in 'C:\\\\Users\\\\jf79\\\\OneDrive - Office Shared Service\\\\Documents\\\\H&F Analysis\\\\Fuel Poverty Analysis\\\\Fuel Poverty General\\\\Fuel Bible': ['certificates.csv', 'columns.csv', 'LICENCE.txt', 'recommendations.csv', 'schema.json', 'UPRN_to_LSOA_HF.csv']\n"
     ]
    }
   ],
   "source": [
    "# Database credentials\n",
    "db_host = 'LBHHLWSQL0001.lbhf.gov.uk'\n",
    "db_port = '1433'\n",
    "db_name = 'IA_ODS'\n",
    " \n",
    "# Create the connection string for SQL Server using pyodbc with Windows Authentication\n",
    "connection_string = f'mssql+pyodbc://@{db_host}:{db_port}/{db_name}?driver=ODBC+Driver+17+for+SQL+Server&Trusted_Connection=yes'\n",
    "\n",
    "# Create the database engine\n",
    "engine = create_engine(connection_string)\n",
    "\n",
    "# Define the current working directory\n",
    "cwd = r'C:\\Users\\jf79\\OneDrive - Office Shared Service\\Documents\\H&F Analysis\\Fuel Poverty Analysis\\Fuel Poverty General\\Fuel Bible'\n",
    "os.chdir(cwd)\n",
    "files = os.listdir(os.getcwd())\n",
    "print(\"Files in %r: %s\" % (cwd, files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your Current Directory is: C:\\Users\\jf79\\OneDrive - Office Shared Service\\Documents\\H&F Analysis\\FOI Sentiment Analysis\\Data\n",
      "Files in: ['FOI Details Data.csv']\n"
     ]
    }
   ],
   "source": [
    "# Read the CSV file into a DataFrame\n",
    "cwd = r'C:\\Users\\jf79\\OneDrive - Office Shared Service\\Documents\\H&F Analysis\\FOI Sentiment Analysis\\Data'\n",
    "os.chdir(cwd)\n",
    "read_directory()\n",
    "columns = [\n",
    "    'ClosedIRKey','CaseType','Division','Service','DateReceived','DateClosed','Due',\n",
    "    'Status','Directorate','Team','Currentstage','Outcome','Outcomedate','ContactMethod',\n",
    "    'Rating','DateAccepted','AssignedTo','Title','FirstName','LastName','Name','Address',\n",
    "    'Town','County','Postcode','Details','TimescaleExtensionReason','ExtensionReason',\n",
    "    'Extensionpermittedbylegislation','Representative'\n",
    "]\n",
    "FOI_details_data = pd.read_csv('FOI Details Data.csv', usecols=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "FOI_details = FOI_details_data.copy()\n",
    "\n",
    "# Data transformations\n",
    "columns = [\n",
    "    'Title','FirstName','LastName','Name',\n",
    "    'Address','Town','County','Postcode'\n",
    "]\n",
    "FOI_address_details = pd.concat(\n",
    "    (FOI_details[['ClosedIRKey']], FOI_details[columns]),\n",
    "    axis=1\n",
    ")\n",
    "FOI_details = FOI_details.drop(columns=columns)\n",
    "FOI_details['Details'] = FOI_details['Details'].fillna('').astype(str)\n",
    "FOI_details.rename(columns={'Details':'details'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Transformation for Sentiment Analysis\n",
    "FOI_request = FOI_details[['details']].copy()\n",
    "\n",
    "# Apply processing to 'details' and 'summary' columns\n",
    "FOI_request['details'] = FOI_request['details'].apply(process_text)\n",
    "\n",
    "# Tokenize 'details' and 'summary' columns\n",
    "FOI_request['tokens'] = pd.DataFrame(FOI_request['details'].apply(safe_tokenize))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:  ['authority', 'local', 'school', 'year', 'question', 'number', 'child', 'area', 'total', 'data']\n",
      "Topic 1:  ['contract', 'contact', 'date', 'address', 'copy', 'data', 'record', 'london', 'service', 'fund']\n",
      "Topic 2:  ['council', 'property', 'road', 'housing', 'list', 'business', 'notice', 'rate', 'address', 'scheme']\n",
      "Topic 3:  ['care', 'service', 'child', 'social', 'home', 'childrens', 'provider', 'adult', 'number', 'support']\n",
      "Topic 4:  ['year', 'council', 'number', 'total', '2022', '2021', '2023', 'housing', 'financial', 'application']\n",
      "0\n",
      "authority: 2034.28\n",
      "local: 1855.76\n",
      "school: 1340.20\n",
      "year: 927.04\n",
      "question: 503.66\n",
      "number: 437.44\n",
      "child: 414.24\n",
      "area: 412.10\n",
      "total: 320.21\n",
      "data: 272.54\n",
      "1\n",
      "contract: 1326.09\n",
      "contact: 982.46\n",
      "date: 953.51\n",
      "address: 839.48\n",
      "copy: 839.26\n",
      "data: 822.71\n",
      "record: 685.04\n",
      "london: 582.73\n",
      "service: 567.16\n",
      "fund: 539.18\n",
      "2\n",
      "council: 1575.13\n",
      "property: 1517.04\n",
      "road: 846.32\n",
      "housing: 525.04\n",
      "list: 518.19\n",
      "business: 512.49\n",
      "notice: 512.41\n",
      "rate: 502.07\n",
      "address: 495.56\n",
      "scheme: 493.46\n",
      "3\n",
      "care: 1604.19\n",
      "service: 1440.47\n",
      "child: 1210.16\n",
      "social: 1095.96\n",
      "home: 829.62\n",
      "childrens: 548.16\n",
      "provider: 502.25\n",
      "adult: 496.19\n",
      "number: 484.90\n",
      "support: 478.69\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'Dictionary' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 26\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mword\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscore\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Assuming `corpus` is your preprocessed list of tokenized documents\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Create a dictionary from the corpus\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m id2word \u001b[38;5;241m=\u001b[39m \u001b[43mDictionary\u001b[49m(FOI_request[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtokens\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Create a bag-of-words corpus from the corpus\u001b[39;00m\n\u001b[0;32m     29\u001b[0m bow_corpus \u001b[38;5;241m=\u001b[39m [id2word\u001b[38;5;241m.\u001b[39mdoc2bow(text) \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m FOI_request[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtokens\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Dictionary' is not defined"
     ]
    }
   ],
   "source": [
    "# Vectorise text\n",
    "vectorizer = CountVectorizer(max_features=100, stop_words='english')\n",
    "X = vectorizer.fit_transform(FOI_request['details'])\n",
    "\n",
    "# Fit LDA model\n",
    "lda = LatentDirichletAllocation(n_components=5, random_state=42)\n",
    "lda.fit(X)\n",
    "\n",
    "for idx, topic in enumerate(lda.components_):\n",
    "    print(f\"Topic {idx}: \", [vectorizer.get_feature_names_out()[i] for i in topic.argsort()[-10:][::-1]])\n",
    "\n",
    "# Example for Topic 0\n",
    "for i in range(0,4):\n",
    "    print(i)\n",
    "    topic_idx = i\n",
    "    word_contributions = {vectorizer.get_feature_names_out()[i]: lda.components_[topic_idx][i]\n",
    "                        for i in range(len(vectorizer.get_feature_names_out()))}\n",
    "    sorted_contributions = sorted(word_contributions.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Display top 10 contributing words\n",
    "    for word, score in sorted_contributions[:10]:\n",
    "        print(f\"{word}: {score:.2f}\")\n",
    "\n",
    "# Assuming `corpus` is your preprocessed list of tokenized documents\n",
    "# Create a dictionary from the corpus\n",
    "id2word = Dictionary(FOI_request['tokens'])\n",
    " \n",
    "# Create a bag-of-words corpus from the corpus\n",
    "bow_corpus = [id2word.doc2bow(text) for text in FOI_request['tokens']]\n",
    " \n",
    "coherence_scores = []\n",
    "perplexities = []\n",
    " \n",
    "topic_numbers = range(1, 6)\n",
    " \n",
    "for n in topic_numbers:\n",
    "    # Sklearn LDA model\n",
    "    lda = LatentDirichletAllocation(n_components=n, random_state=42)\n",
    "    lda.fit(X)  # X should be a term-frequency matrix (e.g., from CountVectorizer or TfidfVectorizer)\n",
    "    # Perplexity for sklearn LDA\n",
    "    perplexities.append(lda.perplexity(X))\n",
    "    # Gensim LDA model\n",
    "    lda_gensim = LdaModel(corpus=bow_corpus, id2word=id2word, num_topics=n, random_state=42)\n",
    "    # Coherence using gensim's LDA model\n",
    "    coherence_model = CoherenceModel(model=lda_gensim, texts=FOI_request['tokens'], dictionary=id2word, coherence='c_v')\n",
    "    coherence_scores.append(coherence_model.get_coherence())\n",
    " \n",
    "# Plotting results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(topic_numbers, coherence_scores, label='Coherence Score (C_v)', marker='o')\n",
    "plt.plot(topic_numbers, perplexities, label='Perplexity', marker='x')\n",
    "plt.xlabel('Number of Topics')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Coherence and Perplexity for Topic Validation')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the tokenized details\n",
    "all_tokens = [token for sublist in FOI_request['tokens'] for token in sublist]\n",
    "# Count word frequencies\n",
    "word_counts = Counter(all_tokens)\n",
    "# Print the top 10 most common words\n",
    "word_counts = pd.DataFrame([word_counts])\n",
    "word_counts.reset_index(inplace=True)\n",
    "word_counts = word_counts.melt(id_vars='index',var_name='Word',value_name='Count')\n",
    "word_counts.sort_values(by='Count',ascending=False, inplace=True)\n",
    "word_counts.reset_index(inplace=True)\n",
    "word_counts[word_counts['Count'] > 100]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
