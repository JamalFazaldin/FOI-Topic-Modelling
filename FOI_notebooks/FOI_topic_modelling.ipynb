{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imported Libaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "cwd = 'C:\\Users\\jf79\\Repos\\FOI-Topic-Modelling'\n",
    "import myfuncs.myfuncs as mf\n",
    "import myfuncs.repofuncs as rf\n",
    "import myfuncs.plotfuncs as pf\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Script Specifc Imported Libaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import itertools\n",
    "\n",
    "import threading\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "from queue import Queue\n",
    "results_queue = Queue()\n",
    "\n",
    "\n",
    "\n",
    "import nltk\n",
    "nltk_data_path = os.getenv('', 'default/path')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import spacy\n",
    "from spellchecker import SpellChecker\n",
    "from wordsegment import Segmenter\n",
    "\n",
    "# Initialize external tools\n",
    "spell = SpellChecker()\n",
    "custom_segmenter = Segmenter()\n",
    "custom_segmenter.load()  # Load data for wordsegment\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "vocabulary = [\n",
    "    'FOI','vapes','vape','riley'\n",
    "]\n",
    "custom_stopwords = {\n",
    "    'freedom','information','act','following','report',\n",
    "    'please','dear','yes','request','provide',\n",
    "    'hammersmith','fulham','like','email','requesting',\n",
    "    'see','attach','foi','attached','service','would','many',\n",
    "    'faithfully','number','council','authority','local','year',\n",
    "    'borough','name','could','data','within','question','including',\n",
    "    'copy','also','made','regarding','since','relating','requested',\n",
    "    'thank','confirm','sent','make','date','much','kind','regard',\n",
    "    'good','afternoon','evening','sir','madam','hello','per','correspondence',\n",
    "    'address','total','contact','look','forward','hearing','way','day','apologise',\n",
    "    'responding','grateful','necessary','period','list','time','related','detail',\n",
    "    'use','end','last'\n",
    "}\n",
    "month_stopwords = {\n",
    "    'january','february','march','april',\n",
    "    'may','june','july','august','september',\n",
    "    'october','november','december','jan','feb',\n",
    "    'mar','apr','may','jun','jul','aug','sep',\n",
    "    'oct','nov','dec','sept'\n",
    "}\n",
    "\n",
    "spell.word_frequency.load_words(vocabulary)\n",
    "for word in vocabulary:\n",
    "    custom_segmenter.unigrams[word] = 1e9 # Ensuring words in the vocabulary are being recognised in the Segmenter\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.update(custom_stopwords)\n",
    "stop_words.update(month_stopwords)\n",
    "\n",
    "import gensim\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "import pickle\n",
    "import pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file into a DataFrame\n",
    "cwd = r'C:\\Users\\jf79\\OneDrive - Office Shared Service\\Documents\\H&F Analysis\\FOI Topic Modelling\\Data'\n",
    "os.chdir(cwd)\n",
    "mf.read_directory()\n",
    "columns = [\n",
    "    'ClosedIRKey','CaseType','Division','Service','DateReceived','DateClosed','Due',\n",
    "    'Status','Directorate','Team','Currentstage','Outcome','Outcomedate','ContactMethod',\n",
    "    'Rating','DateAccepted','AssignedTo','Title','FirstName','LastName','Name','Address',\n",
    "    'Town','County','Postcode','Details','TimescaleExtensionReason','ExtensionReason',\n",
    "    'Extensionpermittedbylegislation','Representative'\n",
    "]\n",
    "FOI_details_data = pd.read_csv('FOI Details Data.csv', usecols=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FOI_details = FOI_details_data.copy()\n",
    "\n",
    "# Data transformations\n",
    "columns = [\n",
    "    'Title','FirstName','LastName','Name',\n",
    "    'Address','Town','County','Postcode'\n",
    "]\n",
    "FOI_address_details = pd.concat(\n",
    "    (FOI_details[['ClosedIRKey']], FOI_details[columns]),\n",
    "    axis=1\n",
    ")\n",
    "FOI_details = FOI_details.drop(columns=columns)\n",
    "FOI_details['Details'] = FOI_details['Details'].fillna('').astype(str)\n",
    "FOI_details.rename(columns={'Details':'details'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Transformation for Topic Modelling\n",
    "FOI_request = FOI_details[['details']].copy()\n",
    "\n",
    "# Apply processing to 'details' and 'summary' columns\n",
    "FOI_request['details'] = FOI_request['details'].apply(rf.process_text)\n",
    "\n",
    "# Tokenize 'details' and 'summary' columns\n",
    "FOI_request['tokens'] = pd.DataFrame(FOI_request['details'].apply(rf.safe_tokenize))\n",
    "\n",
    "mf.export_to_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file into a DataFrame\n",
    "cwd = r'C:\\Users\\jf79\\OneDrive - Office Shared Service\\Documents\\H&F Analysis\\FOI Sentiment Analysis\\Data'\n",
    "os.chdir(cwd)\n",
    "mf.read_directory()\n",
    "\n",
    "processed_fois = pd.read_csv('processed_fois.csv')\n",
    "FOI_email_data = pd.read_excel('FOI Details Data (2024).xlsx')\n",
    "email_lookup_data = pd.read_excel('Email Domain Lookup.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FOI_email = FOI_email_data.copy()\n",
    "FOI_email = email_lookup_data.copy()\n",
    "\n",
    "\n",
    "processed_fois.columns = processed_fois.columns.str.lower().str.strip().str.replace(' ','_')\n",
    "FOI_email.columns = FOI_email.columns.str.lower().str.strip().str.replace(' ','_')\n",
    "FOI_email.rename(columns={'caseid':'caseid_final'},inplace=True)\n",
    "processed_fois = processed_fois[processed_fois['processed_details'] != '']\n",
    "processed_fois['processed_tokens'] = processed_fois['processed_details'].apply(rf.safe_tokenize)\n",
    "processed_fois['new_tokens'] = processed_fois['processed_tokens'].apply(lambda x: [word for word in x if word not in stop_words])\n",
    "processed_fois = pd.merge(\n",
    "    processed_fois, FOI_email,\n",
    "    how='left', left_on='caseid', right_on='caseid_final',\n",
    "    suffixes=('', '_drop')\n",
    ")\n",
    "processed_fois.drop([col for col in processed_fois.columns if '_drop' in col], axis=1, inplace=True)\n",
    "# count_words(processed_fois['new_tokens'])\n",
    "processed_fois = processed_fois[processed_fois['caseid_final'].isna() == False]\n",
    "processed_fois['email'] = processed_fois['email'].fillna('')\n",
    "\n",
    "# Apply the function to the email column\n",
    "processed_fois['domain'] = processed_fois['email'].apply(rf.extract_domain)\n",
    "processed_fois[['group','domain']] = processed_fois['domain'].str.split('.',n=1, expand=True)\n",
    "columns_to_drop = ['processed_tokens','new_tokens']\n",
    "columns = [x for x in processed_fois.columns.to_list() if x not in columns_to_drop]\n",
    "processed_fois = pd.merge(\n",
    "    processed_fois, FOI_email,\n",
    "    how='left', left_on='group', right_on='domain',\n",
    "    suffixes=('', '_drop')\n",
    ")\n",
    "processed_fois['category'] = processed_fois['category'].fillna('Unknown') \n",
    "mf.export_to_csv(processed_fois)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    " \n",
    "# Count the occurrences of each unique pair\n",
    "counts = processed_fois.groupby(['category', 'directorate']).size().reset_index(name='Value')\n",
    " \n",
    "# Create a mapping of unique labels to indices\n",
    "unique_labels = list(set(counts['category'].tolist() + counts['directorate'].tolist()))\n",
    "label_to_index = {label: i for i, label in enumerate(unique_labels)}\n",
    " \n",
    "# Map category and directorate to their indices\n",
    "counts['categoryIndex'] = counts['category'].map(label_to_index)\n",
    "counts['directorateIndex'] = counts['directorate'].map(label_to_index)\n",
    " \n",
    "# Prepare data for Sankey diagram\n",
    "category = counts['categoryIndex'].tolist()\n",
    "directorate = counts['directorateIndex'].tolist()\n",
    "value = counts['Value'].tolist()\n",
    " \n",
    "# Create Sankey diagram\n",
    "fig = go.Figure(data=[go.Sankey(\n",
    "    node=dict(\n",
    "        pad=7,\n",
    "        thickness=20,\n",
    "        line=dict(color=\"black\", width=0.01),\n",
    "        label=unique_labels\n",
    "    ),\n",
    "    link=dict(\n",
    "        source=category,\n",
    "        target=directorate,\n",
    "        value=value\n",
    "    )\n",
    ")])\n",
    " \n",
    "fig.update_layout(\n",
    "    title_text=\"Sankey Diagram\", font_size=10,\n",
    "    width=1000,\n",
    "    height=800    \n",
    ")\n",
    "fig.show()\n",
    "\n",
    "fig.write_html('sankey_diagram.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FOI_corpus = rf.prepare_corpus(FOI_final['new_tokens'],threshold=50)\n",
    "\n",
    "# Create Dictionary\n",
    "id2words = Dictionary(FOI_corpus)\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2words.doc2bow(text) for text in FOI_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize variables\n",
    "results = pd.DataFrame(columns=['Validation_Set', 'Topics', 'Alpha', 'Beta', 'Coherence'])\n",
    "lock = threading.Lock()  # Thread-safe lock\n",
    "\n",
    "# Number of documents in the corpus\n",
    "num_of_docs = len(corpus)\n",
    "\n",
    "# Prepare corpus sets (75% and 100% corpus)\n",
    "corpus_sets = [\n",
    "    gensim.utils.ClippedCorpus(corpus, int(num_of_docs * 0.75)),\n",
    "    corpus\n",
    "]\n",
    "\n",
    "# Hyper-parameters\n",
    "min_topics = 2\n",
    "max_topics = 15\n",
    "step_size = 1\n",
    "topics_range = list(range(min_topics,max_topics+1,step_size))\n",
    "alphas = np.round(np.linspace(0.01, 0.91, 5), 2).tolist() + ['symmetric', 'asymmetric']\n",
    "betas = np.round(np.linspace(0.01, 0.91, 5), 2).tolist() + ['symmetric']\n",
    "corpus_titles = ['75% Corpus', '100% Corpus']\n",
    "\n",
    "# Define test combinations to run in parallel\n",
    "test_combinations = [\n",
    "    (rf.find_element(corpus_title),topic,alpha,beta,corpus_title)\n",
    "    for topic,alpha,beta,corpus_title, in itertools.product(topics_range,alphas,betas,corpus_titles)\n",
    "]\n",
    "\n",
    "tasks = [lambda combination=combination: rf.compute_for_params(*combination) for combination in test_combinations]\n",
    "# Process tasks in parallel with a progress bar\n",
    "with tqdm(total=len(test_combinations), desc='Processing combinations') as pbar:\n",
    "    def task_with_progress(task):\n",
    "        task()\n",
    "        pbar.update(1)\n",
    "    \n",
    "    rf.run_io_task_in_parallel([lambda task=task: task_with_progress(task) for task in tasks])\n",
    "\n",
    "while not results_queue.empty():\n",
    "    result = results_queue.get()\n",
    "    results = pd.concat([results, pd.DataFrame([result])], ignore_index=True)\n",
    "\n",
    "display(results)\n",
    "mf.export_to_csv(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = 6\n",
    "# Build LDA model\n",
    "lda_model = gensim.models.LdaMulticore(\n",
    "    corpus=corpus,\n",
    "    id2word=id2words,\n",
    "    num_topics=num_topics,\n",
    "    random_state=42,\n",
    "    chunksize=100,\n",
    "    passes=10,\n",
    "    alpha=0.24,\n",
    "    eta=0.91\n",
    ")\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=FOI_corpus, dictionary=id2words, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('Coherence Score: ', coherence_lda)\n",
    "# Assuming lda_model is your trained LdaModel\n",
    "lda_model.save(f\"C:/Users/jf79/OneDrive - Office Shared Service/Documents/H&F Analysis/FOI Sentiment Analysis/Models/lda_model ({num_topics})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the topics\n",
    "pyLDAvis.enable_notebook()\n",
    "path = 'C:/Users/jf79/OneDrive - Office Shared Service/Documents/H&F Analysis/FOI Sentiment Analysis/Data'\n",
    "LDAvis_data_filepath = os.path.join(path+str(num_topics))\n",
    "\n",
    "# # this is a bit time consuming - make the if statement True\n",
    "# # if you want to execute visualization prep yourself\n",
    "if 1 == 1:\n",
    "    LDAvis_prepared = gensimvis.prepare(lda_model, corpus, id2words)\n",
    "    with open(LDAvis_data_filepath, 'wb') as f:\n",
    "        pickle.dump(LDAvis_prepared, f)\n",
    "\n",
    "# load the pre-prepared pyLDAvis data from disk\n",
    "with open(LDAvis_data_filepath, 'rb') as f:\n",
    "    LDAvis_prepared = pickle.load(f)\n",
    "\n",
    "pyLDAvis.save_html(LDAvis_prepared,path+str(num_topics)+'.html')\n",
    "\n",
    "LDAvis_prepared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambd = 0.2  # A specific relevance metric value\n",
    " \n",
    "all_topics = {}\n",
    "all_topics_with_docs = {}\n",
    "num_topics = lda_model.num_topics\n",
    "num_terms = 30\n",
    "num_docs = 5  # Number of most related documents to retrieve for each topic\n",
    " \n",
    "topic_doc_matrix = lda_model.get_document_topics(corpus, minimum_probability=0)  # Get the topic-document distribution\n",
    "print\n",
    "for i in range(1, num_topics + 1):  # Indices are 1-based, not 0-based\n",
    "    # Extract the term information\n",
    "    topic = LDAvis_prepared.topic_info[LDAvis_prepared.topic_info.Category == 'Topic' + str(i)].copy()\n",
    "    topic['relevance'] = topic['loglift'] * (1 - lambd) + topic['logprob'] * lambd\n",
    "    all_topics['Topic ' + str(i)] = topic.sort_values(by='relevance', ascending=False).Term[:num_terms].values\n",
    " \n",
    "    # Find the most related documents for the topic\n",
    "    topic_index = i - 1  # Convert to 0-based index\n",
    "    doc_relevance = [doc[topic_index][1] for doc in topic_doc_matrix]  # Extract the relevance of documents for this topic\n",
    "    top_doc_indices = sorted(range(len(doc_relevance)), key=lambda x: doc_relevance[x], reverse=True)\n",
    "    sorted_doc_relevance = [doc_relevance[i] for i in top_doc_indices]\n",
    "    \n",
    "    # Retrieve the actual document texts\n",
    "    top_docs_content = [FOI_final['details'][idx] for idx in top_doc_indices]  # Assuming `documents` contains the original texts\n",
    "    all_topics_with_docs['Topic ' + str(i)] = {\n",
    "        'terms': all_topics['Topic ' + str(i)],\n",
    "        'top_docs_indices': top_doc_indices,  # Indice of most relevant doc\n",
    "        'top_doc_relevance': sorted_doc_relevance,  # relevance of doc\n",
    "        # 'top_docs': top_docs_content  # Contents of the most relevant documents\n",
    "    }\n",
    " \n",
    "# Create a DataFrame to display terms and documents for each topic\n",
    "output_data = {\n",
    "    'Topic': [],\n",
    "    'Top Terms': [],\n",
    "    'Top Doc Indices': [],\n",
    "    'Top Doc Relevance': [],\n",
    "    # 'Top Documents': []\n",
    "}\n",
    "\n",
    "for topic, data in all_topics_with_docs.items():\n",
    "    output_data['Topic'].append(topic)\n",
    "    output_data['Top Terms'].append(\", \".join(data['terms']))\n",
    "    output_data['Top Doc Indices'].append(\", \".join(map(str,data['top_docs_indices'])))\n",
    "    output_data['Top Doc Relevance'].append(\", \".join(map(str,data['top_doc_relevance'])))\n",
    "    # output_data['Top Documents'].append(\" || \".join(data['top_docs']))  # Separate documents with '||'\n",
    "\n",
    "output_data = pd.DataFrame(output_data)\n",
    "mf.export_to_csv(output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = output_data.copy()\n",
    "FOI_topics = FOI_final.copy()\n",
    "# Step 1: Convert 'col1' and 'col2' into lists\n",
    "output['Top Doc Indices List'] = output['Top Doc Indices'].str.split(',')\n",
    "output['Top Doc Relevance List'] = output['Top Doc Relevance'].str.split(',')\n",
    "\n",
    "# Step 2: Create a new column that combines both lists into a dictionary\n",
    "output['combined_dict'] = output.apply(lambda row: dict(zip(row['Top Doc Indices List'], row['Top Doc Relevance List'])), axis=1)\n",
    "output.drop(['Top Doc Indices List','Top Doc Relevance List','Top Doc Indices','Top Doc Relevance', 'Top Terms'],axis=1,inplace=True)\n",
    "output.set_index('Topic',inplace=True)\n",
    "\n",
    "expanded_rows = output['combined_dict'].apply(pd.Series).stack().reset_index()\n",
    "expanded_rows['index'] = expanded_rows['level_1'].astype('Int64')\n",
    "\n",
    "columns = [\n",
    "    'caseid',\n",
    "    'directorate',\n",
    "    'category'\n",
    "]\n",
    "FOI_topics = FOI_topics[columns]\n",
    "FOI_topics.reset_index(inplace=True)\n",
    "FOI_merged = pd.merge(\n",
    "    expanded_rows, FOI_topics,\n",
    "    how='left', on='index',\n",
    "    suffixes=('', '_drop')\n",
    ")\n",
    "FOI_merged.drop(columns=['level_1','index'],inplace=True)\n",
    "FOI_merged.rename(columns={0:'Relevance'},inplace=True)\n",
    "FOI_merged['Relevance'] = FOI_merged['Relevance'].astype(float)\n",
    "\n",
    "topic_names = {\n",
    "    'Topic 1' : 'Housing and Support',\n",
    "    'Topic 2' : 'Contract Management',\n",
    "    'Topic 3' : 'Education and Schools',\n",
    "    'Topic 4' : 'Legal and Regulatory',\n",
    "    'Topic 5' : 'Retail and Consumer Issues',\n",
    "    'Topic 6' : 'Property and Business'\n",
    "}\n",
    "FOI_merged['Topic'] = FOI_merged['Topic'].map(topic_names)\n",
    "FOI_merged\n",
    "pf.sankey3_diagram(FOI_merged,source='category',intermediate='Topic',target='directorate', value='Relevance', name='DIRECTORATE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_doc_matrix = lda_model.get_document_topics(corpus, minimum_probability=0)  # Get the topic-document distribution\n",
    "for i in range(1, num_topics + 1):  # Indices are 1-based, not 0-based\n",
    "    # Find the most related documents for the topic\n",
    "    topic_index = i - 1  # Convert to 0-based index\n",
    "    doc_relevance = [doc[topic_index][1] for doc in topic_doc_matrix]  # Extract the relevance of documents for this topic\n",
    "    top_doc_indices = sorted(range(len(doc_relevance)), key=lambda x: doc_relevance[x], reverse=True)[:num_docs]\n",
    "    print(doc_relevance[0])\n",
    "    print(f'{topic_index}: {topic_doc_matrix[0][topic_index][1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_relevance = [doc[topic_index][1] for doc in topic_doc_matrix]\n",
    "topic_doc_matrix[0][0][1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
